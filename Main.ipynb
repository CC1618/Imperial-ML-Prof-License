{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836056d0",
   "metadata": {},
   "source": [
    "# Portfolio project on optimising a model for real-life data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f367a9",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Predict peaks and troughs in financial assets prices time series using only technical indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc68ea",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2fcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d6e3bd",
   "metadata": {},
   "source": [
    "### Import features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb2791",
   "metadata": {},
   "source": [
    "We won't perform the preprocessing of the data here, we will be working directly with the features and the responses pre calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189871b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('equity_data.csv', index_col='date', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f19ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>px_last</th>\n",
       "      <th>px_high</th>\n",
       "      <th>px_low</th>\n",
       "      <th>px_open</th>\n",
       "      <th>px_volume</th>\n",
       "      <th>asset</th>\n",
       "      <th>rsi_7</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>rsi_28</th>\n",
       "      <th>roc_1m</th>\n",
       "      <th>roc_3m</th>\n",
       "      <th>bb_1m</th>\n",
       "      <th>px_dev_21d</th>\n",
       "      <th>px_dev_50d</th>\n",
       "      <th>ma_3d_stoch_14d</th>\n",
       "      <th>ma_15d_stoch_70d</th>\n",
       "      <th>psycho_12d</th>\n",
       "      <th>psycho_8d</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-05-31</th>\n",
       "      <td>361.23</td>\n",
       "      <td>361.84</td>\n",
       "      <td>360.23</td>\n",
       "      <td>360.86</td>\n",
       "      <td>119856200.0</td>\n",
       "      <td>SPX Index</td>\n",
       "      <td>73.837822</td>\n",
       "      <td>72.009660</td>\n",
       "      <td>66.788579</td>\n",
       "      <td>8.722348</td>\n",
       "      <td>8.562241</td>\n",
       "      <td>0.792335</td>\n",
       "      <td>2.883125</td>\n",
       "      <td>5.109125</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.290838</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-06-01</th>\n",
       "      <td>363.16</td>\n",
       "      <td>363.52</td>\n",
       "      <td>361.21</td>\n",
       "      <td>361.26</td>\n",
       "      <td>141159800.0</td>\n",
       "      <td>SPX Index</td>\n",
       "      <td>77.751141</td>\n",
       "      <td>74.077085</td>\n",
       "      <td>68.059651</td>\n",
       "      <td>8.574504</td>\n",
       "      <td>8.231507</td>\n",
       "      <td>0.826827</td>\n",
       "      <td>3.032047</td>\n",
       "      <td>5.526882</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.588718</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-06-04</th>\n",
       "      <td>367.40</td>\n",
       "      <td>367.85</td>\n",
       "      <td>362.43</td>\n",
       "      <td>363.16</td>\n",
       "      <td>133370100.0</td>\n",
       "      <td>SPX Index</td>\n",
       "      <td>83.917001</td>\n",
       "      <td>77.933232</td>\n",
       "      <td>70.621292</td>\n",
       "      <td>9.485353</td>\n",
       "      <td>10.085695</td>\n",
       "      <td>0.927410</td>\n",
       "      <td>3.788661</td>\n",
       "      <td>6.562560</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.588718</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-06-05</th>\n",
       "      <td>366.64</td>\n",
       "      <td>368.78</td>\n",
       "      <td>365.49</td>\n",
       "      <td>367.40</td>\n",
       "      <td>158007000.0</td>\n",
       "      <td>SPX Index</td>\n",
       "      <td>79.320115</td>\n",
       "      <td>75.757878</td>\n",
       "      <td>69.583932</td>\n",
       "      <td>8.348355</td>\n",
       "      <td>8.495842</td>\n",
       "      <td>0.878514</td>\n",
       "      <td>3.181850</td>\n",
       "      <td>6.160949</td>\n",
       "      <td>98.109453</td>\n",
       "      <td>98.457897</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-06-06</th>\n",
       "      <td>364.96</td>\n",
       "      <td>366.64</td>\n",
       "      <td>364.42</td>\n",
       "      <td>366.64</td>\n",
       "      <td>126588400.0</td>\n",
       "      <td>SPX Index</td>\n",
       "      <td>69.501495</td>\n",
       "      <td>71.037515</td>\n",
       "      <td>67.317151</td>\n",
       "      <td>7.174111</td>\n",
       "      <td>8.312806</td>\n",
       "      <td>0.804155</td>\n",
       "      <td>2.373892</td>\n",
       "      <td>5.507518</td>\n",
       "      <td>91.819172</td>\n",
       "      <td>98.135466</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            px_last  px_high  px_low  px_open    px_volume      asset  \\\n",
       "date                                                                    \n",
       "1990-05-31   361.23   361.84  360.23   360.86  119856200.0  SPX Index   \n",
       "1990-06-01   363.16   363.52  361.21   361.26  141159800.0  SPX Index   \n",
       "1990-06-04   367.40   367.85  362.43   363.16  133370100.0  SPX Index   \n",
       "1990-06-05   366.64   368.78  365.49   367.40  158007000.0  SPX Index   \n",
       "1990-06-06   364.96   366.64  364.42   366.64  126588400.0  SPX Index   \n",
       "\n",
       "                rsi_7     rsi_14     rsi_28    roc_1m     roc_3m     bb_1m  \\\n",
       "date                                                                         \n",
       "1990-05-31  73.837822  72.009660  66.788579  8.722348   8.562241  0.792335   \n",
       "1990-06-01  77.751141  74.077085  68.059651  8.574504   8.231507  0.826827   \n",
       "1990-06-04  83.917001  77.933232  70.621292  9.485353  10.085695  0.927410   \n",
       "1990-06-05  79.320115  75.757878  69.583932  8.348355   8.495842  0.878514   \n",
       "1990-06-06  69.501495  71.037515  67.317151  7.174111   8.312806  0.804155   \n",
       "\n",
       "            px_dev_21d  px_dev_50d  ma_3d_stoch_14d  ma_15d_stoch_70d  \\\n",
       "date                                                                    \n",
       "1990-05-31    2.883125    5.109125       100.000000         98.290838   \n",
       "1990-06-01    3.032047    5.526882       100.000000         98.588718   \n",
       "1990-06-04    3.788661    6.562560       100.000000         98.588718   \n",
       "1990-06-05    3.181850    6.160949        98.109453         98.457897   \n",
       "1990-06-06    2.373892    5.507518        91.819172         98.135466   \n",
       "\n",
       "            psycho_12d  psycho_8d  resp  \n",
       "date                                     \n",
       "1990-05-31   66.666667       75.0     2  \n",
       "1990-06-01   75.000000       75.0     0  \n",
       "1990-06-04   83.333333       75.0     0  \n",
       "1990-06-05   75.000000       62.5     0  \n",
       "1990-06-06   66.666667       62.5     0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34201c20",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47763eb",
   "metadata": {},
   "source": [
    "The parameters below specifies:\n",
    "- The subset of equity indices we consider. \n",
    "- Whether we want to predict peaks and/or troughs.\n",
    "- The list of base classifiers and their range of hyper parameters we want to optimise on.\n",
    "- The list of meta classifiers and their range of hyper parameters we want to optimise on.\n",
    "\n",
    "The fine tuning of the parameters will be done with a Bayesian optimisation algorithm provided by Optuna. Each classifier can have its own data processing, for instance some algorithms like the distance based ones for instance (e.g. K-NN) need to get the features on the same scale if we don't specify features preferences. Hence we can specify the algorithm to perform that data transformation for that classifier and adapt the data to algorithm. In order to reduce the dimension we even have the possibility to select fearures in a systematic way by using SelectPercentile or PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcf7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = \\\n",
    "{\n",
    "'is_on':\n",
    "    {\n",
    "    'classifiers':\n",
    "        {\n",
    "        'train': True,\n",
    "        'fit': True,\n",
    "        'eval': True,\n",
    "        },\n",
    "    },\n",
    "'assets':\n",
    "    {\n",
    "    'equity': [\n",
    "               #'DAX Index',\n",
    "               #'HSI Index',\n",
    "               #'MXASJ Index',\n",
    "               #'MXEF Index',\n",
    "               #'MXWD Index',\n",
    "               #'NDX Index',\n",
    "               #'NKY Index',\n",
    "               #'RTY Index',\n",
    "               #'SGX Index',\n",
    "               #'SHSZ300 Index',\n",
    "               'SPX Index',\n",
    "               #'SVX Index',\n",
    "               #'SXXE Index',\n",
    "               #'SXXP Index',\n",
    "               #'UKX Index'\n",
    "               ],\n",
    "    },\n",
    "'training':\n",
    "    {\n",
    "    'train_date': '11/2018',\n",
    "    'test_date': '01/2019',\n",
    "    'classifiers':\n",
    "        {\n",
    "        'algos':\n",
    "            {\n",
    "            'xgb':\n",
    "               {\n",
    "               'standardisation': None, # 'std', None\n",
    "               'normalisation': None, # 'norm', 'minmax', None\n",
    "               'features_selection': None, # 'perc', 'pca', None\n",
    "               'classifier': 'xgb',\n",
    "               },\n",
    "          #'lgb':\n",
    "             #{\n",
    "             #'standardisation': None, # 'std', None\n",
    "             #'normalisation': None, # 'norm', 'minmax', None\n",
    "             #'features_selection': None, # 'perc', 'pca', None\n",
    "             #'classifier': 'lgb',\n",
    "             #},\n",
    "            },\n",
    "        'seed': 42,\n",
    "        'evaluation_metric': 'precision', # 'f1', 'mcc', 'log_loss', 'precision'\n",
    "        'split':\n",
    "            {\n",
    "            'n_splits': 5,\n",
    "            'group_gap': 21,\n",
    "            'max_train_group_size': 252 * 12,\n",
    "            'max_test_group_size': 252 * 4,\n",
    "            },\n",
    "        'features_selection':\n",
    "            {\n",
    "            'perc':\n",
    "                {\n",
    "                'fix_params':\n",
    "                    {\n",
    "                    'score_func': 'f_classif',\n",
    "                    'percentile': 75,\n",
    "                    },\n",
    "                'trials_range':\n",
    "                    {\n",
    "                    #'percentile':  {'low': 10, 'high': 100, 'step': 10},\n",
    "                    },\n",
    "                },\n",
    "            'pca':\n",
    "                {\n",
    "                'fix_params':\n",
    "                    {\n",
    "                    'svd_solver': 'full',\n",
    "                    },\n",
    "                'trials_range':\n",
    "                    {\n",
    "                    'n_components':  {'low': .55, 'high': .95, 'step': .1},\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        'classifier':\n",
    "            {\n",
    "            'xgb':\n",
    "                {\n",
    "                'optimisation': {'n_trials': 25, 'timeout': None},\n",
    "                'fix_params':\n",
    "                    {\n",
    "                    'objective': 'multi:softprob',\n",
    "                    'random_state': 42,\n",
    "                    'importance_type': 'weight',\n",
    "                    #'max_depth': 3,\n",
    "                    #'min_child_weight': 3,\n",
    "                    #'learning_rate': .01,\n",
    "                    #'gamma': 20,\n",
    "                    #'colsample_bytree': .5,\n",
    "                    },\n",
    "                'trials_range':\n",
    "                    {\n",
    "                    'n_estimators':      {'low': 50, 'high': 350, 'step': 50},\n",
    "                    'max_depth':         {'low': 3, 'high': 6, 'step': 1},\n",
    "                    'min_child_weight':  {'low': 6, 'high': 10, 'step': 1},\n",
    "                    'learning_rate':     {'low': .01, 'high': .31, 'step': .05},\n",
    "                    'gamma':             {'low': 8, 'high': 12, 'step': 1},\n",
    "                    'colsample_bytree':  {'low': .4, 'high': .8, 'step': .1},\n",
    "                    'subsample':         {'low': .4, 'high': 1., 'step': .1},\n",
    "                    'colsample_bylevel': {'low': .2, 'high': 1., 'step': .1},\n",
    "                    'colsample_bynode':  {'low': .2, 'high': 1., 'step': .1},\n",
    "                    #'reg_lambda':        {'low': 0., 'high': 1., 'step': .1},\n",
    "                    #'reg_alpha':         {'low': 0., 'high': 1., 'step': .1},\n",
    "                    },\n",
    "                },\n",
    "            'lgb':\n",
    "                {\n",
    "                'optimisation': {'n_trials': 1, 'timeout': None},\n",
    "                'fix_params':\n",
    "                    {\n",
    "                    'objective': 'multiclass',\n",
    "                    'random_state': 42,\n",
    "                    'class_weight': 'balanced',\n",
    "                    #'learning_rate': .01,\n",
    "                    #'max_depth': 2,\n",
    "                    #'num_leaves': 1_000,\n",
    "                    },\n",
    "                'trials_range':\n",
    "                    {\n",
    "                    #'max_depth':        {'low': 2, 'high': 6, 'step':2},\n",
    "                    #'num_leaves':       {'low': 2, 'high': 2**10},\n",
    "                    #'learning_rate':    {'low': .001, 'high': .6, 'log':True},\n",
    "                    #'colsample_bytree': {'low': .4, 'high': 1., 'step': .1},\n",
    "                    #'subsample':        {'low': .4, 'high': 1., 'step': .1},\n",
    "                    },\n",
    "                },\n",
    "            'svm':\n",
    "                {\n",
    "                'optimisation':  {'n_trials': 1, 'timeout': None},\n",
    "                'fix_params':\n",
    "                    {\n",
    "                    'random_state': 42,\n",
    "                    'class_weight': 'balanced',\n",
    "                    'kernel': 'sigmoid',\n",
    "                    #'degree': 17,\n",
    "                    #'max_iter': 500,\n",
    "                    'C': .01,\n",
    "                    },\n",
    "                'trials_range':\n",
    "                    {\n",
    "                    #'C': {'low': 2**-2, 'high': 2**2, 'log': True}, # default value (i.e., 1) works best\n",
    "                    #'kernel': {'choices': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']},\n",
    "                    #'kernel': {'choices': ['rbf', 'poly']},\n",
    "                    #'degree': {'low': 1, 'high': 5, 'step': 2},\n",
    "                    #'gamma': {'low': 2**-2, 'high': 2**2, 'log': True},\n",
    "                    #'shrinking': {'choices': [True, False]},\n",
    "                    #'coef0': {'low': -.1, 'high': .1, 'step': .01}\n",
    "                    },\n",
    "                },\n",
    "            'rf':\n",
    "                {\n",
    "                'optimisation':  {'n_trials': 1, 'timeout': None},\n",
    "                'fix_params':\n",
    "                    {\n",
    "                    'random_state': 42,\n",
    "                    #'max_depth': 3,\n",
    "                    #'max_features': .3,\n",
    "                    #'n_estimators': 1000,\n",
    "                    #'min_samples_split': 20,\n",
    "                    #'max_leaf_nodes': 40,\n",
    "                    #'min_samples_leaf': 20,\n",
    "                    #'max_samples': .5,\n",
    "                    #'max_features': 'log2', #sqrt (default), log2, None\n",
    "                    },\n",
    "                'trials_range':\n",
    "                    {\n",
    "                    #'n_estimators': {'low': 50, 'high': 1000, 'step': 50}, # 650\n",
    "                    #'max_depth': {'low': 3, 'high': 9, 'step': 2}, # 3\n",
    "                    #'max_features': {'low': .1, 'high': .9, 'step': .2}, # .9\n",
    "                    #'min_samples_split': {'low': 2, 'high': 100, 'step': 2}, # .82\n",
    "                    #'max_leaf_nodes': {'low': 2, 'high': 50, 'step': 2},\n",
    "                    #'min_samples_leaf': {'low': 1, 'high': 391, 'step':10},\n",
    "                    #'max_samples': {'low': .1, 'high': 1.},\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc74ddf",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11cf76e",
   "metadata": {},
   "source": [
    "The function below split the data in train and test. When we will perform the optimisation of the hyper parameters of the classifiers we will split the train set into training and validation subset. We need to make sure there is no linkage from the test set and the train set otherwise the evaluation might be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c23519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, params):\n",
    "    \"\"\"\n",
    "    Split the data in:\n",
    "    - X_train: features with dates up to train_date.\n",
    "    - y_train: responses with dates up to train_date.\n",
    "    - X_test: features with dates from test_date.\n",
    "    - y_test: responses with dates from test_date.\n",
    "    \"\"\"\n",
    "    def split(train_test):\n",
    "        date = params['training'][f'{train_test}_date']\n",
    "        if train_test == 'train':\n",
    "            df = data.loc[:date]\n",
    "        else:\n",
    "            df = data.loc[date:]\n",
    "        X_ = df.loc[:,'asset':].drop(['resp'], axis=1)\n",
    "        y_ = df[['asset', 'resp']]\n",
    "        return X_, y_\n",
    "\n",
    "    data = data.loc[data.asset.isin(params['assets']['equity'])]\n",
    "    X_train, y_train = split('train')\n",
    "    X_test, y_test = split('test')\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4aef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(data, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3943c83",
   "metadata": {},
   "source": [
    "### Machine Learning Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dc36d",
   "metadata": {},
   "source": [
    "This script gathers a collection of machine learning functions. For instance \"build_pipeline\" provide a nice way to add different steps of data tranformation before applying the classification. \"apply_frac_diff\" deals with the non stationarity of time series by using fractional differenciation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92cafaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "def build_pipeline(params):\n",
    "    \"\"\"\n",
    "    Build the sklearn pipeline with the following steps:\n",
    "    - standardisation: Standardize features by removing the mean \n",
    "      and scaling to unit variance.\n",
    "    - Normalisation: make the features between 0 and 1. \n",
    "    - Feature selection: reduce the number of features with either \n",
    "      a PCA or SelectPercentile which select features according to \n",
    "      a percentile of the highest scores.\n",
    "    - Classifier: Predict the response based on the transformed \n",
    "      features. \n",
    "    \"\"\"\n",
    "    ML_library = \\\n",
    "        {\n",
    "        'standardisation':\n",
    "            {\n",
    "            'std': StandardScaler,\n",
    "            },\n",
    "        'normalisation':\n",
    "            {\n",
    "            'norm': Normalizer,\n",
    "            'minmax': MinMaxScaler,\n",
    "            },\n",
    "        'features_selection':\n",
    "            {\n",
    "            'perc': SelectPercentile,\n",
    "            'pca': PCA,\n",
    "            },\n",
    "        'classifier':\n",
    "            {\n",
    "            'xgb': xgb.XGBClassifier,\n",
    "            'lgb': LGBMClassifier,\n",
    "            'rf': RandomForestClassifier,\n",
    "            'svm': SVC,\n",
    "            'knn': KNeighborsClassifier,\n",
    "            'lr': LogisticRegression,\n",
    "            },\n",
    "        }\n",
    "    l_steps = []\n",
    "    for k, v in params.items():\n",
    "        if v['name'] is not None:\n",
    "            l_steps.append((k, ML_library[k][v['name']](**v['params'])))\n",
    "    pipe = Pipeline(l_steps)\n",
    "    return pipe\n",
    "\n",
    "def get_params_fitting(cls_algo, y_wght):\n",
    "    \"\"\"\n",
    "    Complete the arguments of the classifier to take account the \n",
    "    weights of the reponses. \n",
    "    \"\"\"\n",
    "    d_params = \\\n",
    "    {\n",
    "    'xgb':\n",
    "        {\n",
    "        'classifier__sample_weight': y_wght,\n",
    "        'classifier__verbose': False,\n",
    "        },\n",
    "    'rf':\n",
    "        {\n",
    "        'classifier__sample_weight': y_wght,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if cls_algo in d_params:\n",
    "        return d_params[cls_algo]\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def get_sample_weight(y):\n",
    "    \"\"\"\n",
    "    Estimate sample weights by class for unbalanced datasets.\n",
    "    Responses with few occurences will get higher weight.\n",
    "    \"\"\" \n",
    "    l_y_wght = compute_sample_weight('balanced', y)\n",
    "    return pd.Series(l_y_wght, index=y.index)\n",
    "\n",
    "def apply_frac_diff(ts, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply fractional differential to make the data stationary\n",
    "    with loosing too much information. \n",
    "    \"\"\"\n",
    "    def get_wght(d, thres):\n",
    "        w, k=[1.], 1\n",
    "        while True:\n",
    "            w_= -w[-1] * (d - k + 1) / k\n",
    "            if abs(w_) < thres:break\n",
    "            w.append(w_)\n",
    "            k += 1\n",
    "        return np.array(w[::-1]).reshape(-1,1)\n",
    "\n",
    "    def apply_wght(ts, d, thres):\n",
    "        w = get_wght(d, thres)\n",
    "        width = len(w) - 1\n",
    "        d_res = {}\n",
    "        if width >= ts.shape[0]:raise Exception(\"width is oversize\")\n",
    "        for i in range(width, ts.shape[0]):\n",
    "            i_0_index, i_1_index = ts.index[i-width], ts.index[i]\n",
    "            data = np.dot(w.T, ts.loc[i_0_index:i_1_index])[0]\n",
    "            d_res[i_1_index] = data\n",
    "        return pd.Series(d_res)\n",
    "\n",
    "    def get_adf_corr(ts):\n",
    "        out = pd.DataFrame(columns=['adfStat','pVal','lags',\\\n",
    "                                 'nb_obs','95% conf','corr'])\n",
    "        for d in np.linspace(0, 1 , 11):\n",
    "            ts_trans = apply_wght(ts, d, kwargs['thres'])\n",
    "            corr = np.corrcoef(ts.loc[ts_trans.index], ts_trans)[0,1]\n",
    "            adf = adfuller(ts_trans, maxlag=1, regression='c',autolag=None)\n",
    "            out.loc[d] = list(adf[:4])+[adf[4]['5%']]+[corr]\n",
    "        return out\n",
    "\n",
    "    if 'thres' not in kwargs: kwargs['thres'] = 1e-4\n",
    "    out = get_adf_corr(ts)\n",
    "    min_d = min(out.loc[out.pVal <= .05].index)\n",
    "    res = apply_wght(ts, min_d, kwargs['thres'])\n",
    "    res.index.name = 'date'\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9972a",
   "metadata": {},
   "source": [
    "PurgedGroupTimeSeriesSplit assures non leakage between the training and the validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8dcf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if groups[idx] in group_dict:\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "\n",
    "                train_array = np.sort(np.unique(\n",
    "                    np.concatenate((train_array,\n",
    "                                    train_array_tmp)),\n",
    "                    axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for test_group_idx in unique_groups[group_test_start:group_test_start + group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                    np.concatenate((test_array,\n",
    "                                    test_array_tmp)),\n",
    "                    axis=None), axis=None)\n",
    "\n",
    "            test_array = test_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                pass\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c6671",
   "metadata": {},
   "source": [
    "ClassifierHPO use Optuna's bayesian optmisation algorithm to find the best set of parameters values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d0ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import f1_score, log_loss, matthews_corrcoef, precision_score\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "\n",
    "class ClassifierHPO():\n",
    "    \"\"\"\n",
    "    Classifier Hyper Parameters Optimisation: Find the best set of \n",
    "    hyper parameters values, that gives the lowest error on the \n",
    "    validation set. \n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, y_wght, algo, params):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.y_wght = y_wght\n",
    "        self.algo = algo\n",
    "        self.params = params\n",
    "        self.best_params = None\n",
    "        self.main()\n",
    "\n",
    "    def get_params_trial(self, trial):\n",
    "        \"\"\"\n",
    "        Provides the set of hyper parameters values tested by Optuna's algorithm. \n",
    "        \"\"\"\n",
    "        def get_params_tr(key, value):\n",
    "            def get_params_fs_perc(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'percentile': trial.suggest_float,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            def get_params_fs_pca(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'n_components': trial.suggest_float,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            def get_params_xgb(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'n_estimators': trial.suggest_int,\n",
    "                    'max_depth': trial.suggest_int,\n",
    "                    'min_child_weight': trial.suggest_int,\n",
    "                    'learning_rate': trial.suggest_float,\n",
    "                    'reg_lambda': trial.suggest_int,\n",
    "                    'reg_alpha': trial.suggest_int,\n",
    "                    'subsample': trial.suggest_float,\n",
    "                    'colsample_bytree': trial.suggest_float,\n",
    "                    'colsample_bylevel': trial.suggest_float,\n",
    "                    'colsample_bynode': trial.suggest_float,\n",
    "                    'gamma': trial.suggest_int,\n",
    "                    'grow_policy': trial.suggest_categorical,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            def get_params_lgb(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'max_depth': trial.suggest_int,\n",
    "                    'num_leaves': trial.suggest_int,\n",
    "                    'learning_rate': trial.suggest_float,\n",
    "                    'colsample_bytree': trial.suggest_float,\n",
    "                    'subsample': trial.suggest_float,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            def get_params_rf(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'max_leaf_nodes': trial.suggest_int,\n",
    "                    'n_estimators': trial.suggest_int,\n",
    "                    'max_depth': trial.suggest_int,\n",
    "                    'min_samples_split': trial.suggest_int,\n",
    "                    'min_samples_leaf': trial.suggest_int,\n",
    "                    'max_samples': trial.suggest_float,\n",
    "                    'min_weight_fraction_leaf': trial.suggest_float,\n",
    "                    'max_features': trial.suggest_float,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            def get_params_svm(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'C': trial.suggest_float,\n",
    "                    'kernel': trial.suggest_categorical,\n",
    "                    'degree': trial.suggest_int,\n",
    "                    'gamma': trial.suggest_float,\n",
    "                    'shrinking': trial.suggest_categorical,\n",
    "                    'coef0': trial.suggest_float,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            def get_params_knn(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'n_neighbors': trial.suggest_int,\n",
    "                    'weights': trial.suggest_categorical,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            def get_params_lr(trial):\n",
    "                d_suggest = \\\n",
    "                    {\n",
    "                    'penalty': trial.suggest_categorical,\n",
    "                    'C': trial.suggest_float,\n",
    "                    }\n",
    "                return {x: d_suggest[x](x, **params_tr[x]) for x in params_tr}\n",
    "\n",
    "            if self.params_opt[key] is not None:\n",
    "                params_tr = self.params_opt[key]['trials_range']\n",
    "                d_params_tr = {'perc': get_params_fs_perc,\n",
    "                               'pca': get_params_fs_pca,\n",
    "                               'xgb': get_params_xgb,\n",
    "                               'lgb': get_params_lgb,\n",
    "                               'rf': get_params_rf,\n",
    "                               'svm': get_params_svm,\n",
    "                               'knn': get_params_knn,\n",
    "                               'lr': get_params_lr,}\n",
    "                res = d_params_tr[d_k[key]](trial)\n",
    "                res = self.params_opt[key]['fix_params'] | res\n",
    "\n",
    "                d_fs_scr = {'f_classif': f_classif}\n",
    "                if key == 'features_selection' and value == 'perc':\n",
    "                    res['score_func'] = d_fs_scr[res['score_func']]\n",
    "                return res\n",
    "\n",
    "        params_trial = {}\n",
    "\n",
    "        d_k = {'features_selection': self.algo['features_selection'],\n",
    "               'classifier': self.algo['classifier']}\n",
    "\n",
    "        for k, v in self.algo.items():\n",
    "            if v is not None:\n",
    "                if k in ['features_selection', 'classifier']:\n",
    "                    params_trial[k] = {'name': v,\n",
    "                                       'params': get_params_tr(k, v)}\n",
    "                else:\n",
    "                    params_trial[k] = {'name': v,\n",
    "                                       'params': {}}\n",
    "        return params_trial\n",
    "\n",
    "    def get_perf(self, pipe,\n",
    "                       X_training,\n",
    "                       y_training,\n",
    "                       X_validation,\n",
    "                       y_validation,\n",
    "                       y_wght_training,\n",
    "                       y_wght_validation):\n",
    "        d_mtx = \\\n",
    "        {\n",
    "        'mcc': matthews_corrcoef,\n",
    "        'f1': f1_score,\n",
    "        'log_loss': log_loss,\n",
    "        'precision': precision_score,\n",
    "        }\n",
    "\n",
    "        pipe.fit(X_training.values,\n",
    "                 y_training.squeeze(),\n",
    "                 **get_params_fitting(self.algo['classifier'], y_wght_training))\n",
    "\n",
    "        y_pred = pipe.predict(X_validation.values)\n",
    "        return d_mtx[self.params['evaluation_metric']](y_validation,\n",
    "                                                       y_pred,\n",
    "                                                       sample_weight=y_wght_validation,\n",
    "                                                       average='weighted')\n",
    "    def get_params_opt(self):\n",
    "        params_opt = {}\n",
    "        for k, v in self.algo.items():\n",
    "            if v is not None and k in ['features_selection', 'classifier']:\n",
    "                params_opt[k] = self.params[k][v]\n",
    "        return params_opt\n",
    "\n",
    "    def main(self):\n",
    "        def objective(trial):\n",
    "            params_trial = self.get_params_trial(trial)\n",
    "            self.params_trial_record[trial.number] = params_trial\n",
    "            pipe = build_pipeline(params_trial)\n",
    "            l_perf = []\n",
    "            split = self.cv.split(self.X, groups=self.X.index.values)\n",
    "            for i, (training_idx, validation_idx) in enumerate(split):\n",
    "                X_training = self.X.iloc[training_idx]\n",
    "                y_training = self.y.iloc[training_idx]\n",
    "                X_validation = self.X.iloc[validation_idx]\n",
    "                y_validation = self.y.iloc[validation_idx]\n",
    "                y_wght_training = self.y_wght.iloc[training_idx]\n",
    "                y_wght_validation = self.y_wght.iloc[validation_idx]\n",
    "\n",
    "                validation_start_date = max(X_training.index).strftime('%Y-%m-%d')\n",
    "                X_validation = X_validation.sort_index().loc[validation_start_date:]\n",
    "                y_validation = y_validation.sort_index().loc[validation_start_date:]\n",
    "                y_wght_validation = y_wght_validation.sort_index().loc[validation_start_date:]\n",
    "\n",
    "                perf = self.get_perf(pipe,\n",
    "                                     X_training,\n",
    "                                     y_training,\n",
    "                                     X_validation,\n",
    "                                     y_validation,\n",
    "                                     y_wght_training,\n",
    "                                     y_wght_validation)\n",
    "                l_perf.append(perf)\n",
    "            return round(np.median(l_perf), 4)\n",
    "\n",
    "        self.cv = PurgedGroupTimeSeriesSplit(**self.params['split'])\n",
    "        self.params_opt = self.get_params_opt()\n",
    "\n",
    "        sampler = TPESampler(seed=self.params['seed'])\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "        self.params_trial_record = {}\n",
    "        study.optimize(objective, **self.params_opt['classifier']['optimisation'])\n",
    "        self.best_params = self.params_trial_record[study.best_trial.number]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b782fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import plotext as plt\n",
    "import pandas as pd\n",
    "\n",
    "class EvalClassif():\n",
    "    \"\"\"\n",
    "    Evaluate the classifier on the test by plotting a lift chart,\n",
    "    a confusion matrix, and the main error statistics. \n",
    "    \"\"\"\n",
    "    def __init__(self, y_true, y_pred, y_wght, target_names=None):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.y_wght = y_wght\n",
    "        self.target_names = target_names\n",
    "\n",
    "    def plot_lift_chart(self):\n",
    "        df = pd.concat([self.y_true.astype('int'), self.y_pred],\n",
    "                       axis=1)\n",
    "        df.columns = ['true', 'pred']\n",
    "        df = df.sort_values('pred')\n",
    "        df['model'] = df['true'].cumsum()\n",
    "        ratio = max(df['model']) / len(df.index)\n",
    "        df['random'] = [x * ratio for x in range(1, len(df.index) + 1)]\n",
    "        df['oracle'] = df['true'].sort_values(ascending=False).cumsum().to_list()\n",
    "        df = df[['model', 'random', 'oracle']]\n",
    "        df = df.reset_index(drop=True)\n",
    "        plt.plot(df['model'])\n",
    "        plt.plot(df['random'])\n",
    "        plt.plot(df['oracle'])\n",
    "        plt.plot_size(40, 10)\n",
    "        plt.show()\n",
    "        plt.clear_data()\n",
    "        plt.clear_figure()\n",
    "\n",
    "    def plot_conf_matrix(self):\n",
    "        plt.confusion_matrix(self.y_true.squeeze(), self.y_pred, labels = self.target_names, color='dark')\n",
    "        plt.plot_size(60, 10)\n",
    "        plt.show()\n",
    "        plt.clear_figure()\n",
    "\n",
    "    def main(self):\n",
    "        print(classification_report(self.y_true,\n",
    "                                    self.y_pred,\n",
    "                                    target_names=self.target_names))\n",
    "        self.plot_conf_matrix()\n",
    "        #self.plot_lift_chart()\n",
    "\n",
    "    def get_pt_score(self):\n",
    "        conf = confusion_matrix(self.y_true, self.y_pred)\n",
    "        return conf[0][0] + conf[2][2]**2/conf[1][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999f31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    \"\"\"\n",
    "    Gathers the main methods to optimise the hyper parameters of the classifiers, fit the model and evaluate it. \n",
    "    \"\"\"\n",
    "    def __init__(self, algo_id, params):\n",
    "        self.algo_id = algo_id\n",
    "        self.params = params\n",
    "\n",
    "    def optimise_hyperparams(self, X, y, y_wght, algo_d):\n",
    "        y_wght = y_wght.drop(columns='asset')\n",
    "        return ClassifierHPO(X, y, y_wght, algo_d, self.params).best_params\n",
    "\n",
    "    def train(self, X, y):\n",
    "        y_wght = get_sample_weight(y)\n",
    "        algo_pipe = self.params['algos'][self.algo_id]\n",
    "        self.mdl_params = self.optimise_hyperparams(X, y, y_wght, algo_pipe)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_wght = get_sample_weight(y)\n",
    "        pipe = build_pipeline(self.mdl_params)\n",
    "        pipe.fit(X.values,\n",
    "                 y.squeeze(),\n",
    "                 **get_params_fitting(self.mdl_params['classifier']['name'], y_wght))\n",
    "        self.mdl = pipe\n",
    "        return pipe\n",
    "\n",
    "    def predict(self, X):\n",
    "        return pd.Series(self.mdl.predict(X.values), index=X.index, name='y_pred')\n",
    "\n",
    "    def evaluate(self, y_true, y_pred, target_names):\n",
    "        y_wght = get_sample_weight(y_true)\n",
    "        EvalClassif(y_true, y_pred, y_wght, target_names).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ceb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Storage():\n",
    "    \"\"\"\n",
    "    Gathers the main classes to store the model's parameters, the fitted model and the results. \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_storage_dir(dir):\n",
    "        storage_dir = os.getcwd() + '/storage/'\n",
    "        if dir is not None: storage_dir += f'{dir}/'\n",
    "        try:\n",
    "            os.makedirs(storage_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        return storage_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def to_pickle(obj, name, dir=None):\n",
    "        storage_dir = Storage.get_storage_dir(dir)\n",
    "        with open(f'{storage_dir}{name}.pickle', 'wb') as handle:\n",
    "            pickle.dump(obj, handle)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pickle(name, dir=None):\n",
    "        storage_dir = Storage.get_storage_dir(dir)\n",
    "        with open(f'{storage_dir}{name}.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_csv(df, name, dir=None):\n",
    "        storage_dir = Storage.get_storage_dir(dir)\n",
    "        df.to_csv(f'{storage_dir}/{name}.csv')\n",
    "\n",
    "    @staticmethod\n",
    "    def from_csv(name, dir=None):\n",
    "        storage_dir = Storage.get_storage_dir(dir)\n",
    "        return pd.read_csv(f'{storage_dir}/{name}.csv',\n",
    "                           index_col='date',\n",
    "                           parse_dates=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f08a780b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-08 16:27:14,968]\u001b[0m A new study created in memory with name: no-name-26035659-6d9d-4a3c-a38e-9b0cfd4d4e25\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************* Classification Algo: xgb *********************************\n",
      "~~~~~~~~~~ Training ~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-08 16:27:16,806]\u001b[0m Trial 0 finished with value: 0.7956 and parameters: {'n_estimators': 150, 'max_depth': 6, 'min_child_weight': 9, 'learning_rate': 0.21000000000000002, 'gamma': 8, 'colsample_bytree': 0.4, 'subsample': 0.4, 'colsample_bylevel': 0.9000000000000001, 'colsample_bynode': 0.7}. Best is trial 0 with value: 0.7956.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:18,795]\u001b[0m Trial 1 finished with value: 0.788 and parameters: {'n_estimators': 250, 'max_depth': 3, 'min_child_weight': 10, 'learning_rate': 0.26, 'gamma': 9, 'colsample_bytree': 0.4, 'subsample': 0.5, 'colsample_bylevel': 0.4, 'colsample_bynode': 0.6000000000000001}. Best is trial 0 with value: 0.7956.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:20,881]\u001b[0m Trial 2 finished with value: 0.7697 and parameters: {'n_estimators': 200, 'max_depth': 4, 'min_child_weight': 9, 'learning_rate': 0.01, 'gamma': 9, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'colsample_bylevel': 0.9000000000000001, 'colsample_bynode': 0.30000000000000004}. Best is trial 0 with value: 0.7956.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:23,444]\u001b[0m Trial 3 finished with value: 0.8056 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.21000000000000002, 'gamma': 8, 'colsample_bytree': 0.4, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bynode': 0.9000000000000001}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:24,877]\u001b[0m Trial 4 finished with value: 0.7924 and parameters: {'n_estimators': 150, 'max_depth': 3, 'min_child_weight': 9, 'learning_rate': 0.16000000000000003, 'gamma': 8, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.4, 'colsample_bylevel': 1.0, 'colsample_bynode': 0.4}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:28,231]\u001b[0m Trial 5 finished with value: 0.7823 and parameters: {'n_estimators': 250, 'max_depth': 4, 'min_child_weight': 8, 'learning_rate': 0.16000000000000003, 'gamma': 8, 'colsample_bytree': 0.8, 'subsample': 0.9, 'colsample_bylevel': 1.0, 'colsample_bynode': 1.0}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:31,114]\u001b[0m Trial 6 finished with value: 0.7845 and parameters: {'n_estimators': 250, 'max_depth': 6, 'min_child_weight': 6, 'learning_rate': 0.060000000000000005, 'gamma': 8, 'colsample_bytree': 0.5, 'subsample': 0.6000000000000001, 'colsample_bylevel': 0.4, 'colsample_bynode': 0.9000000000000001}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:32,814]\u001b[0m Trial 7 finished with value: 0.7588 and parameters: {'n_estimators': 150, 'max_depth': 4, 'min_child_weight': 8, 'learning_rate': 0.01, 'gamma': 12, 'colsample_bytree': 0.4, 'subsample': 1.0, 'colsample_bylevel': 0.8, 'colsample_bynode': 0.30000000000000004}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:33,900]\u001b[0m Trial 8 finished with value: 0.7812 and parameters: {'n_estimators': 50, 'max_depth': 6, 'min_child_weight': 9, 'learning_rate': 0.26, 'gamma': 11, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'colsample_bylevel': 0.30000000000000004, 'colsample_bynode': 0.9000000000000001}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:36,860]\u001b[0m Trial 9 finished with value: 0.7798 and parameters: {'n_estimators': 250, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.11, 'gamma': 9, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.8, 'colsample_bylevel': 0.9000000000000001, 'colsample_bynode': 0.6000000000000001}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:40,979]\u001b[0m Trial 10 finished with value: 0.7921 and parameters: {'n_estimators': 350, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.31, 'gamma': 10, 'colsample_bytree': 0.6000000000000001, 'subsample': 1.0, 'colsample_bylevel': 0.7, 'colsample_bynode': 0.8}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:42,388]\u001b[0m Trial 11 finished with value: 0.7943 and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.21000000000000002, 'gamma': 10, 'colsample_bytree': 0.5, 'subsample': 0.4, 'colsample_bylevel': 0.7, 'colsample_bynode': 0.7}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:44,386]\u001b[0m Trial 12 finished with value: 0.7857 and parameters: {'n_estimators': 150, 'max_depth': 6, 'min_child_weight': 10, 'learning_rate': 0.21000000000000002, 'gamma': 8, 'colsample_bytree': 0.4, 'subsample': 0.8, 'colsample_bylevel': 0.6000000000000001, 'colsample_bynode': 0.7}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:48,467]\u001b[0m Trial 13 finished with value: 0.7872 and parameters: {'n_estimators': 350, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.21000000000000002, 'gamma': 9, 'colsample_bytree': 0.5, 'subsample': 0.6000000000000001, 'colsample_bylevel': 1.0, 'colsample_bynode': 0.5}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:49,781]\u001b[0m Trial 14 finished with value: 0.7845 and parameters: {'n_estimators': 50, 'max_depth': 6, 'min_child_weight': 9, 'learning_rate': 0.31, 'gamma': 11, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.8, 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:52,202]\u001b[0m Trial 15 finished with value: 0.8002 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.11, 'gamma': 8, 'colsample_bytree': 0.4, 'subsample': 0.5, 'colsample_bylevel': 0.9000000000000001, 'colsample_bynode': 0.8}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:55,613]\u001b[0m Trial 16 finished with value: 0.7975 and parameters: {'n_estimators': 300, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.11, 'gamma': 9, 'colsample_bytree': 0.5, 'subsample': 0.5, 'colsample_bylevel': 0.5, 'colsample_bynode': 0.8}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:27:58,058]\u001b[0m Trial 17 finished with value: 0.7871 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.11, 'gamma': 10, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.9, 'colsample_bylevel': 0.2, 'colsample_bynode': 0.9000000000000001}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:28:01,087]\u001b[0m Trial 18 finished with value: 0.7862 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.060000000000000005, 'gamma': 11, 'colsample_bytree': 0.7000000000000001, 'subsample': 0.7000000000000001, 'colsample_bylevel': 0.8, 'colsample_bynode': 0.8}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:28:04,209]\u001b[0m Trial 19 finished with value: 0.7947 and parameters: {'n_estimators': 300, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.060000000000000005, 'gamma': 8, 'colsample_bytree': 0.4, 'subsample': 0.5, 'colsample_bylevel': 0.7, 'colsample_bynode': 1.0}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:28:06,042]\u001b[0m Trial 20 finished with value: 0.7873 and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.16000000000000003, 'gamma': 12, 'colsample_bytree': 0.8, 'subsample': 0.7000000000000001, 'colsample_bylevel': 0.9000000000000001, 'colsample_bynode': 0.5}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:28:11,398]\u001b[0m Trial 21 finished with value: 0.7975 and parameters: {'n_estimators': 300, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.11, 'gamma': 9, 'colsample_bytree': 0.5, 'subsample': 0.5, 'colsample_bylevel': 0.5, 'colsample_bynode': 0.8}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:28:16,838]\u001b[0m Trial 22 finished with value: 0.7975 and parameters: {'n_estimators': 300, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.11, 'gamma': 9, 'colsample_bytree': 0.5, 'subsample': 0.5, 'colsample_bylevel': 0.6000000000000001, 'colsample_bynode': 0.8}. Best is trial 3 with value: 0.8056.\u001b[0m\n",
      "\u001b[32m[I 2023-06-08 16:28:19,755]\u001b[0m Trial 23 finished with value: 0.7993 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.16000000000000003, 'gamma': 8, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'colsample_bylevel': 0.5, 'colsample_bynode': 0.9000000000000001}. Best is trial 3 with value: 0.8056.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-08 16:28:22,142]\u001b[0m Trial 24 finished with value: 0.7979 and parameters: {'n_estimators': 200, 'max_depth': 4, 'min_child_weight': 7, 'learning_rate': 0.26, 'gamma': 8, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'colsample_bylevel': 0.5, 'colsample_bynode': 0.9000000000000001}. Best is trial 3 with value: 0.8056.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~ Fitting ~~~~~~~~~~\n",
      "..... Evaluation on SPX Index .....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nts       0.97      0.88      0.92       975\n",
      "        peak       0.02      0.10      0.04        20\n",
      "      trough       0.22      0.45      0.30        20\n",
      "\n",
      "    accuracy                           0.86      1015\n",
      "   macro avg       0.40      0.48      0.42      1015\n",
      "weighted avg       0.93      0.86      0.89      1015\n",
      "\n",
      "                         \u001b[0m\u001b[38;5;12m\u001b[1mConfusion Matrix\u001b[0m                   \u001b[0m\n",
      "      \u001b[0m\u001b[38;5;12m┌────────────────────────────────────────────────────┐\u001b[0m\n",
      "      \u001b[0m\u001b[38;5;12m│\u001b[0m\u001b[38;2;80;80;80m█████████████████\u001b[0m\u001b[38;2;236;236;236m█████████████████\u001b[0m\u001b[38;2;247;247;247m▐█████████████████\u001b[0m\u001b[38;5;12m│\u001b[0m\n",
      "\u001b[38;5;12m\u001b[1m   nts\u001b[0m\u001b[38;5;12m┤\u001b[0m\u001b[38;2;249;249;249m▄▄▄▄▄▄▄▄▄\u001b[0m\u001b[48;2;80;80;80m\u001b[38;5;12m\u001b[1m858 - 84.53%\u001b[0m\u001b[38;2;253;253;253m▄▄▄▄▄\u001b[0m\u001b[48;2;236;236;236m\u001b[38;5;12m\u001b[1m85 - 8.37%\u001b[0m\u001b[38;2;253;253;253m▄▄▄▄▄▄▄\u001b[0m\u001b[48;2;247;247;247m\u001b[38;5;12m\u001b[1m32 - 3.15\u001b[0m\u001b[38;5;12m│\u001b[0m\n",
      "\u001b[38;5;12m\u001b[1m  peak\u001b[0m\u001b[38;5;12m┤\u001b[0m\u001b[38;2;249;249;249m█████████\u001b[0m\u001b[48;2;249;249;249m\u001b[38;5;12m\u001b[1m18 - 1.77%\u001b[0m\u001b[38;2;253;253;253m███████\u001b[0m\u001b[48;2;253;253;253m\u001b[38;5;12m\u001b[1m2 - 0.2%\u001b[0m\u001b[38;2;253;253;253m▐████████\u001b[0m\u001b[48;2;253;253;253m\u001b[38;5;12m\u001b[1m0 - 0.0%\u001b[0m\u001b[38;2;253;253;253m█\u001b[0m\u001b[38;5;12m│\u001b[0m\n",
      "\u001b[38;5;12m\u001b[1mtrough\u001b[0m\u001b[38;5;12m┤\u001b[0m\u001b[38;2;251;251;251m█████████\u001b[0m\u001b[48;2;251;251;251m\u001b[38;5;12m\u001b[1m11 - 1.08%\u001b[0m\u001b[38;2;253;253;253m███████\u001b[0m\u001b[48;2;253;253;253m\u001b[38;5;12m\u001b[1m0 - 0.0%\u001b[0m\u001b[38;2;251;251;251m▐████████\u001b[0m\u001b[48;2;251;251;251m\u001b[38;5;12m\u001b[1m9 - 0.89%\u001b[0m\u001b[38;5;12m│\u001b[0m\n",
      "      \u001b[0m\u001b[38;5;12m│\u001b[0m\u001b[38;2;251;251;251m█████████████████\u001b[0m\u001b[38;2;253;253;253m█████████████████\u001b[0m\u001b[38;2;251;251;251m▐█████████████████\u001b[0m\u001b[38;5;12m│\u001b[0m\n",
      "      \u001b[0m\u001b[38;5;12m└─────────┬────────────────┬────────────────┬────────┘\u001b[0m\n",
      "               \u001b[0m\u001b[38;5;12m\u001b[1mnts\u001b[0m             \u001b[0m\u001b[38;5;12m\u001b[1mpeak\u001b[0m            \u001b[0m\u001b[38;5;12m\u001b[1mtrough\u001b[0m       \u001b[0m\n",
      "\u001b[38;5;12m\u001b[1mActual\u001b[0m                       \u001b[0m\u001b[38;5;12m\u001b[1mPredicted\u001b[0m                      \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cls_params = params['training']['classifiers']\n",
    "is_on = params['is_on']\n",
    "asset_class = 'equity'\n",
    "for algo_id in cls_params['algos']:\n",
    "    print('*'*33, f'Classification Algo: {algo_id}', '*'*33)\n",
    "    dir = f'{asset_class}/classifiers/{algo_id}'\n",
    "    cls = Classifier(algo_id,  cls_params)\n",
    "    if is_on['classifiers']['train']:\n",
    "        print('~'*10, 'Training', '~'*10)\n",
    "        cls.train(X_train.drop(columns='asset'),\n",
    "                  y_train.drop(columns='asset'))\n",
    "        Storage.to_pickle(cls.mdl_params, f'{algo_id}_params', dir)\n",
    "    if is_on['classifiers']['fit']:\n",
    "        print('~'*10, 'Fitting', '~'*10)\n",
    "        cls.mdl_params = Storage.from_pickle(f'{algo_id}_params', dir)\n",
    "        mdl = cls.fit(X_train.drop(columns='asset'),\n",
    "                      y_train.drop(columns='asset'))\n",
    "        Storage.to_pickle(mdl, f'{algo_id}_mdl', dir)\n",
    "    if is_on['classifiers']['eval']:\n",
    "        cls.mdl = Storage.from_pickle(f'{algo_id}_mdl', dir)\n",
    "        for asset in params['assets'][asset_class]:\n",
    "            print('.'*5, f'Evaluation on {asset}', '.'*5)\n",
    "            target_names = ['nts', 'peak', 'trough']\n",
    "            X_test_asset = X_test.loc[X_test.asset==asset]\n",
    "            X_test_asset = X_test_asset.drop(columns='asset')\n",
    "            y_true = y_test.loc[y_test.asset==asset]\n",
    "            y_true = y_true.drop(columns='asset')\n",
    "            y_pred = cls.predict(X_test_asset)\n",
    "            cls.evaluate(y_true, y_pred, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a56d19",
   "metadata": {},
   "source": [
    "### How to read the results:\n",
    "\n",
    "- Number of true negatives: number of times their was no peaks and the algo predicted it correctly.\n",
    "- Number of false positives: number of times the algo predicted a peak but it wasn’t right.\n",
    "- Number of false negatives: number of times the algo missed a peak.\n",
    "- Number of true positives: number of times the algo predicted correctly the peaks.\n",
    " \n",
    "The objective is to have the highest number of true positives, and the lowest number of false ones.\n",
    "\n",
    "If you apply the training, fitting, and evaluation only on the SPX Index you observe the peaks are much more difficult to predict than troughs. The error is much higher, and it misses most of the true values. One way to mitigate is to include in the fitting and the other indices. However, when doing so, you will pick most of the peaks which is good, but the signal will be much more sensitive and the number of false positive will go out of the roof. One way to mitigate would be to stack several classifiers togethers as weak learners, and use their outputs as inputs for a strong classifiers, which would learn from the outputs of the weak ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e287b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46924f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784f499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be459cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7ccff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41135f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7bb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:church] *",
   "language": "python",
   "name": "conda-env-church-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
